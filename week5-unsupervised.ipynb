{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":16295,"databundleVersionId":1099992,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-18T06:11:38.366332Z","iopub.execute_input":"2024-08-18T06:11:38.366865Z","iopub.status.idle":"2024-08-18T06:11:39.594563Z","shell.execute_reply.started":"2024-08-18T06:11:38.366816Z","shell.execute_reply":"2024-08-18T06:11:39.593301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the datasets: train.csv, test.csv\ntrain = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tweet-sentiment-extraction/test.csv\")\n\n# Displaying the first few rows of the datasets\nprint(\"Training Data:\")\nprint(train.head())\nprint(\"\\nTest Data:\")\nprint(test.head())","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:12:41.156990Z","iopub.execute_input":"2024-08-18T06:12:41.157435Z","iopub.status.idle":"2024-08-18T06:12:41.354700Z","shell.execute_reply.started":"2024-08-18T06:12:41.157402Z","shell.execute_reply":"2024-08-18T06:12:41.353489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1. Data Collection and Provenance\n\nFor this project, we are working on a sentiment analysis problem using a dataset of tweets with existing sentiment labels. The dataset is publicly available under the Creative Commons Attribution 4.0 International License. \n\n**Data Source:** The data originates from Figure Eight's Data for Everyone platform. Figure Eight specializes in crowdsourced datasets, which are used for tasks such as machine learning, sentiment analysis, and natural language processing. The dataset used here is titled *Sentiment Analysis: Emotion in Text*, which focuses on the extraction of emotions and sentiments from short texts like tweets.\n\n**Dataset Overview:**\n- **train.csv**: Contains tweets, their sentiment labels (positive, negative, or neutral), and a word or phrase that encapsulates the sentiment (selected_text).\n- **test.csv**: Contains tweets and their sentiment labels, but without the selected phrase.\n- **sample_submission.csv**: Provides a sample format for submission, including the tweet ID and the predicted word or phrase that supports the sentiment.\n\n### Provenance of the Data:\nThe data has been collected via crowdsourcing on Figure Eight’s platform, ensuring a diverse set of annotations from multiple contributors. The annotations consist of the sentiment of the tweets and the specific part of the text that highlights the sentiment. The data was originally released as part of a sentiment analysis task and has been adapted for this project.\n\n### Method of Data Collection:\nThe dataset was likely collected using online workers who annotated each tweet by selecting the phrase that best represents its sentiment. These workers provided labels for thousands of tweets, which were aggregated into this dataset for training and evaluation purposes.\n","metadata":{}},{"cell_type":"markdown","source":"## Identifying an Unsupervised Learning Problem\n\n### Problem Description:\n\nWhile the primary goal of this project is to perform sentiment analysis using supervised learning (where we predict the sentiment-supporting phrase based on labeled training data), we can also explore some unsupervised learning aspects to enhance our understanding of the data.\n\n**Clustering for Similarity Detection:**\nIn addition to predicting the sentiment phrases, we can apply unsupervised learning techniques such as clustering to group tweets with similar sentiment words or patterns. This can be useful to detect common themes or topics within the tweets that may not be explicitly labeled.\n\nFor example, tweets with positive sentiment may cluster around certain recurring words or phrases like \"amazing,\" \"love,\" or \"great.\" Similarly, tweets with negative sentiment may cluster around phrases like \"terrible,\" \"hate,\" or \"bad.\" By clustering these tweets based on their text features, we can identify underlying patterns and relationships that may not be immediately obvious.\n\n### Approach:\n\nWe will use a technique like **K-means clustering** or **topic modeling** (e.g., Latent Dirichlet Allocation - LDA) on the tweet texts. This will allow us to group the tweets into clusters based on the similarity of their text, without using the labeled sentiment data. Once the clusters are created, we can analyze whether these clusters align with sentiment labels and extract common themes within each cluster.\n\nThe benefit of clustering is that it may reveal hidden structures in the data, such as specific topics or sub-categories of sentiments, which can provide further insights for understanding tweet sentiment beyond what is directly labeled.\n\n### Methodology:\n\n1. **Text Preprocessing**: We will first preprocess the tweets by removing stop words, punctuation, and performing tokenization and vectorization (e.g., using TF-IDF or word embeddings).\n2. **Clustering Algorithm**: We will apply K-means clustering to group the tweets based on their textual similarity. Alternatively, LDA can be used for topic modeling to find underlying themes.\n3. **Evaluation**: After clustering, we will examine the resulting clusters and identify common words/phrases within each cluster. We will also compare the clusters to the sentiment labels (although not required for unsupervised learning) to see if there are any correlations between the clusters and sentiment categories.\n\nThis approach allows us to investigate patterns in the data and potentially discover insights beyond the labeled dataset, demonstrating an effective application of unsupervised learning.\n","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)\n\n### 1. Inspecting the Dataset\n\nThe first step in the EDA process is to inspect the dataset and understand its structure. We'll look at the data types, check for missing values, and examine the basic statistics for each column.\n","metadata":{}},{"cell_type":"code","source":"# Inspect the first few rows of the training data\ntrain.head()\n\n# Check for missing values and data types\ntrain.info()\n\n# Summary statistics of numerical columns\ntrain.describe()\n\n# Check for unique values in the 'sentiment' column\nprint(\"Sentiment Counts:\")\nprint(train['sentiment'].value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:13:54.799826Z","iopub.execute_input":"2024-08-18T06:13:54.800252Z","iopub.status.idle":"2024-08-18T06:13:54.935883Z","shell.execute_reply.started":"2024-08-18T06:13:54.800219Z","shell.execute_reply":"2024-08-18T06:13:54.934631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Visualizing the Dataset\n\nWe’ll visualize the distribution of sentiments in the dataset and analyze the length of the tweets to gain insights into the data.\n\n#### Sentiment Distribution\nLet's visualize the distribution of sentiments (positive, negative, neutral) to understand the balance of classes in the dataset.\n","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Plotting the distribution of sentiments\nplt.figure(figsize=(8, 5))\nsns.countplot(x='sentiment', data=train)\nplt.title(\"Distribution of Sentiments\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:14:09.604001Z","iopub.execute_input":"2024-08-18T06:14:09.604415Z","iopub.status.idle":"2024-08-18T06:14:10.114956Z","shell.execute_reply.started":"2024-08-18T06:14:09.604384Z","shell.execute_reply":"2024-08-18T06:14:10.113810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Tweet Length Distribution\nLet's calculate the length of each tweet and visualize the distribution. This can help us understand if longer or shorter tweets correlate with certain sentiments.\n","metadata":{}},{"cell_type":"code","source":"# Ensure that there are no NaN values in the 'text' column\ntrain['text'] = train['text'].fillna('')  # Replace NaN with an empty string\n\n# Calculate tweet lengths\ntrain['tweet_length'] = train['text'].apply(len)\n\n# Plotting the distribution of tweet lengths\nplt.figure(figsize=(10, 6))\nsns.histplot(train['tweet_length'], bins=30, kde=True)\nplt.title(\"Distribution of Tweet Lengths\")\nplt.xlabel(\"Tweet Length\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n# Boxplot for tweet length distribution across sentiments\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='sentiment', y='tweet_length', data=train)\nplt.title(\"Tweet Length Distribution by Sentiment\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:21:18.677178Z","iopub.execute_input":"2024-08-18T06:21:18.677653Z","iopub.status.idle":"2024-08-18T06:21:19.677465Z","shell.execute_reply.started":"2024-08-18T06:21:18.677619Z","shell.execute_reply":"2024-08-18T06:21:19.676277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Correlations and Relationships Between Factors\n\nNext, we investigate relationships between factors. In this case, we might look for correlations between tweet length and sentiment or explore relationships between specific words and sentiments.\n\n#### Word Frequency by Sentiment\nWe'll investigate the most frequent words in tweets for each sentiment. This can provide insights into which words are commonly associated with positive, negative, or neutral sentiments.\n","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Function to generate a word cloud for a specific sentiment\ndef generate_wordcloud(sentiment):\n    # Fill NaN values with an empty string and ensure all text entries are strings\n    text = \" \".join(train[train['sentiment'] == sentiment]['text'].fillna(\"\").astype(str))\n    \n    # Generate the word cloud\n    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n    \n    # Display the word cloud\n    plt.figure(figsize=(10, 6))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.title(f\"Word Cloud for {sentiment.capitalize()} Sentiment\")\n    plt.show()\n\n# Generate word clouds for each sentiment\ngenerate_wordcloud('positive')\ngenerate_wordcloud('negative')\ngenerate_wordcloud('neutral')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:39:00.808613Z","iopub.execute_input":"2024-08-18T06:39:00.809741Z","iopub.status.idle":"2024-08-18T06:39:06.624923Z","shell.execute_reply.started":"2024-08-18T06:39:00.809701Z","shell.execute_reply":"2024-08-18T06:39:06.623767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Data Cleaning and Transformation\n\nAfter visualizing and analyzing the data, the next step is cleaning and potentially transforming it. Some key considerations include:\n\n- **Missing Data:** We will check for any missing values in the dataset and decide on how to handle them (e.g., discard or fill in missing values).\n- **Outliers:** We check for outliers in the data, particularly in tweet lengths, and decide whether to remove or transform them.\n- **Text Preprocessing:** We'll clean the tweet text by removing unnecessary characters, stopwords, and performing tokenization and stemming/lemmatization.\n\n#### Handling Missing Data","metadata":{}},{"cell_type":"code","source":"# Check for missing values\nmissing_values = train.isnull().sum()\nprint(\"Missing Values:\\n\", missing_values)\n\n# Dropping rows with missing values in the text or sentiment columns\ntrain = train.dropna(subset=['text', 'sentiment'])","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:14:59.958604Z","iopub.execute_input":"2024-08-18T06:14:59.959569Z","iopub.status.idle":"2024-08-18T06:14:59.993474Z","shell.execute_reply.started":"2024-08-18T06:14:59.959507Z","shell.execute_reply":"2024-08-18T06:14:59.991972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Conclusions from EDA\n\nBased on the EDA, we observe that the sentiment classes are somewhat imbalanced, with more positive tweets than negative or neutral ones. The distribution of tweet lengths varies across sentiments, with neutral tweets tending to be longer than others.\n\nWe identified common words associated with each sentiment using word clouds, and we found that certain positive and negative words are recurrent across the dataset. We also performed essential data cleaning and preprocessing to prepare the text for modeling.\n\nFor further steps, we may need to apply transformations like scaling the data if required by specific models. Moreover, if we find more outliers or missing values in subsequent stages, we may consider more advanced imputation or removal techniques.\n","metadata":{}},{"cell_type":"markdown","source":"## Unsupervised Learning Analysis: Clustering and Dimensionality Reduction\n\n### Purpose\nIn this section, we will apply unsupervised learning techniques to analyze the tweet data. Specifically, we will explore clustering using K-means and hierarchical clustering and also apply dimensionality reduction using Principal Component Analysis (PCA) to visualize the clusters. This will help us explore patterns and structure in the data without the use of sentiment labels.\n","metadata":{}},{"cell_type":"markdown","source":"### Data Preprocessing for Clustering\n\nPurpose:\nWe will preprocess the tweet data by converting the text to numerical features using TF-IDF vectorization. The output will be used as input for the clustering algorithms.\n","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\nnltk.download('omw-1.4')  # Optional, supports additional languages for WordNet\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:44:05.907886Z","iopub.execute_input":"2024-08-18T06:44:05.908859Z","iopub.status.idle":"2024-08-18T06:44:05.978250Z","shell.execute_reply.started":"2024-08-18T06:44:05.908822Z","shell.execute_reply":"2024-08-18T06:44:05.977253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n# Initialize stop words\nstop_words = set(stopwords.words('english'))\n\n# Function to clean and preprocess text without lemmatization\ndef preprocess_text(text):\n    # Remove special characters and digits\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    # Tokenize the text\n    tokens = word_tokenize(text.lower())\n    # Remove stop words\n    cleaned_text = [token for token in tokens if token not in stop_words]\n    return ' '.join(cleaned_text)\n\n# Apply preprocessing to the 'text' column\ntrain['cleaned_text'] = train['text'].apply(preprocess_text)\n\n# Check the first few rows to ensure the cleaned_text column was created\nprint(train[['text', 'cleaned_text']].head())\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:44:19.045598Z","iopub.execute_input":"2024-08-18T06:44:19.046023Z","iopub.status.idle":"2024-08-18T06:44:24.623736Z","shell.execute_reply.started":"2024-08-18T06:44:19.045991Z","shell.execute_reply":"2024-08-18T06:44:24.622414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# TF-IDF Vectorization of the tweet text\nvectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n# Check the available columns in the train dataframe\nprint(train.columns)\n\nX = vectorizer.fit_transform(train['cleaned_text'])\n\n# Shape of the transformed data\nprint(f\"Shape of the TF-IDF matrix: {X.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:44:30.539563Z","iopub.execute_input":"2024-08-18T06:44:30.539966Z","iopub.status.idle":"2024-08-18T06:44:31.077624Z","shell.execute_reply.started":"2024-08-18T06:44:30.539937Z","shell.execute_reply":"2024-08-18T06:44:31.076428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K-means Clustering\n\nPurpose:\nWe apply K-means clustering to the TF-IDF vectorized data. The goal is to group the tweets into clusters based on their content, identifying common patterns and structures in the dataset.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n# Applying K-means clustering\nkmeans = KMeans(n_clusters=3, random_state=42)  # We choose 3 clusters\nkmeans.fit(X)\n\n# Adding cluster labels to the original dataframe\ntrain['kmeans_cluster'] = kmeans.labels_\n\n# Visualizing the number of tweets in each cluster\ntrain['kmeans_cluster'].value_counts().plot(kind='bar')\nplt.title(\"Distribution of Tweets in K-means Clusters\")\nplt.xlabel(\"Cluster\")\nplt.ylabel(\"Number of Tweets\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:44:36.328884Z","iopub.execute_input":"2024-08-18T06:44:36.329309Z","iopub.status.idle":"2024-08-18T06:44:37.704184Z","shell.execute_reply.started":"2024-08-18T06:44:36.329276Z","shell.execute_reply":"2024-08-18T06:44:37.702932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hierarchical Clustering\n\nPurpose:\nWe apply hierarchical clustering to group the tweets based on their similarity. This clustering technique builds a hierarchy of clusters and is often more flexible than K-means for detecting complex patterns in the data.\n","metadata":{}},{"cell_type":"code","source":"from scipy.cluster.hierarchy import dendrogram, linkage\n\n# Generate the linkage matrix for hierarchical clustering\nZ = linkage(X.toarray(), method='ward')\n\n# Plot the dendrogram\nplt.figure(figsize=(10, 7))\ndendrogram(Z, truncate_mode='level', p=3)\nplt.title(\"Dendrogram for Hierarchical Clustering\")\nplt.xlabel(\"Tweet Samples\")\nplt.ylabel(\"Distance\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:44:41.394795Z","iopub.execute_input":"2024-08-18T06:44:41.395213Z","iopub.status.idle":"2024-08-18T06:49:49.135957Z","shell.execute_reply.started":"2024-08-18T06:44:41.395173Z","shell.execute_reply":"2024-08-18T06:49:49.134719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Principal Component Analysis (PCA)\n\nPurpose:\nWe reduce the dimensionality of the TF-IDF data using PCA to visualize the clustering structure. PCA projects the high-dimensional data onto a lower-dimensional space while retaining as much variance as possible.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# Applying PCA to reduce dimensionality\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X.toarray())\n\n# Adding PCA components to the dataframe for visualization\ntrain['pca1'] = X_pca[:, 0]\ntrain['pca2'] = X_pca[:, 1]\n\n# Plotting the clusters in the reduced PCA space\nplt.figure(figsize=(10, 6))\nplt.scatter(train['pca1'], train['pca2'], c=train['kmeans_cluster'], cmap='viridis', s=50)\nplt.title(\"K-means Clusters Visualized in 2D PCA Space\")\nplt.xlabel(\"PCA Component 1\")\nplt.ylabel(\"PCA Component 2\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T06:55:09.360264Z","iopub.execute_input":"2024-08-18T06:55:09.360726Z","iopub.status.idle":"2024-08-18T06:55:11.645044Z","shell.execute_reply.started":"2024-08-18T06:55:09.360691Z","shell.execute_reply":"2024-08-18T06:55:11.643738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Evaluation and Discussion\n\nPurpose:\nWe evaluate the clustering results and provide insights into the clustering structures observed. Additionally, we compare the performance of K-means and hierarchical clustering and discuss the limitations of each approach.\n","metadata":{}},{"cell_type":"markdown","source":"### Discussion:\n\n- **K-means Clustering**: K-means performed well in identifying clusters of tweets. The clusters have clear separation, as observed in the PCA plot. However, K-means assumes that clusters are spherical and evenly sized, which may not always hold true for textual data.\n  \n- **Hierarchical Clustering**: Hierarchical clustering provides a more flexible structure, allowing us to visualize the relationship between clusters. It can reveal more nuanced structures than K-means, especially for non-spherical clusters. However, it is computationally more expensive and can be slow for large datasets.\n\n- **Dimensionality Reduction (PCA)**: The PCA visualization allows us to inspect how well the clusters are separated in a reduced two-dimensional space. This helps confirm that the clusters identified by K-means have meaningful separation based on the tweet content.\n\n### Conclusion:\n\nBoth K-means and hierarchical clustering were useful for exploring the latent structures within the tweet data. K-means offers faster performance and clear clustering, while hierarchical clustering provides more flexibility in detecting complex relationships between clusters. For further analysis, other clustering techniques like DBSCAN could be explored, and model tuning can improve the quality of clustering.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming `test.csv` has a similar structure to `train.csv`\ntest = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/test.csv')\n\n# Predicting with a trained model (placeholder for your prediction logic)\n# For example, using the clustering model's labels (this is just a placeholder logic)\n# You would replace this with the actual logic to select the word or phrase for each tweet\ntest['predicted_selected_text'] = test['text'].apply(lambda x: \"sample prediction\")  # Replace this with your actual predictions\n\n# Creating the submission DataFrame\nsubmission = pd.DataFrame({\n    'textID': test['textID'],\n    'selected_text': test['predicted_selected_text']  # Replace with the actual predicted text or phrase\n})\n\n# Saving the submission DataFrame to a CSV file\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T07:00:33.645578Z","iopub.execute_input":"2024-08-18T07:00:33.646044Z","iopub.status.idle":"2024-08-18T07:00:33.690758Z","shell.execute_reply.started":"2024-08-18T07:00:33.646012Z","shell.execute_reply":"2024-08-18T07:00:33.689447Z"},"trusted":true},"execution_count":43,"outputs":[]}]}